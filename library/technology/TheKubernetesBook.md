# The Kubernetes Book
__By: Nigel Poulton__
## Lessons Learned:
- Kubernetes is like any other cluster – a bunch of machines to host applications. We call these machines “nodes”, and they can be physical servers, virtual machines, cloud instances, Raspberry Pis, and more.
- Orchestrator is just a fancy word for a system that takes care of deploying and managing applications.
- For production environments, multiple control plane nodes configured for high availability (HA) is vital. Generally speaking, 3 or 5 is recommended, and you should spread them across availability zones.
- API server is the Grand Central station of Kubernetes. All communication, between all components, must go through the API server. We’ll get into the detail later, but it’s important to understand that internal system components, as well as external user components, all communicate through the API server – all roads lead to the API Server.
- It exposes a RESTful API that you POST YAML configuration files to over HTTPS. These YAML files, which we sometimes call manifests, describe the desired state of an application. This desired state includes things like which container image to use, which ports to expose, and how many Pod replicas to run.
- The cluster store is the only stateful part of the control plane and persistently stores the entire configuration and state of the cluster.
- The controller manager implements all the background controllers that monitor cluster components and respond to events. Architecturally, the controller manager is a controller of controllers, meaning it spawns all the core controllers and monitors them. Some of the core controllers include the Deployment controller, the StatefulSet controller, and the ReplicaSet controller. Each one is responsible for a small subset of cluster intelligence and runs as a background watch-loop constantly watching the API Server for changes.
- At a high level, the scheduler watches the API server for new work tasks and assigns them to appropriate healthy worker nodes.
- If you’re running your cluster on a supported public cloud platform, such as AWS, Azure, GCP, or Linode, your control plane will be running a cloud controller manager. Its job is to facilitate integrations with cloud services, such as instances, load-balancers, and storage.
- Worker nodes are where user applications run. At a high-level they do three things: Watch the API server for new work assignments Execute work assignments Report back to the control plane (via the API server)
- When you join a node to a cluster, the process installs the kubelet, which is then responsible for registering it with the cluster. This process registers the node’s CPU, memory, and storage into the wider cluster pool.
- Container Runtime Interface (CRI). At a high-level, the CRI masks the internal machinery of Kubernetes and exposes a clean documented interface for 3rd-party container runtimes to plug into.
- kube-proxy. This runs on every node and is responsible for local cluster networking. It ensures each node gets its own unique IP address, and it implements local iptables or IPVS rules to handle routing and load-balancing of traffic on the Pod network.
- the preferred model is to deploy all Pods via higher-level controllers. The most common controller is the Deployment. It offers scalability, self-healing, and rolling updates for stateless apps. You define Deployments in YAML manifest files that specify things like how many replicas to deploy and how to perform updates.
- Manifest files are written in simple YAML and tell Kubernetes what an application should look like. This is called desired state.
- Once you’ve created the manifest, you post it to the API server. A simple way to do this is with the kubectl command-line utility. This sends the manifest to the API Server as an HTTP POST, usually on port 443.
- The simplest model is to run a single container in every Pod. This is why we often use the terms “Pod” and “container” interchangeably. However, there are advanced use-cases that run multiple containers in a single Pod.
- Multi-container Pods are ideal when you have requirements for tightly coupled containers that may need to share memory and storage. However, if you don’t need to tightly couple containers, you should put them in their own Pods and loosely couple them over the network.
- This is where Services come in to play. They provide reliable networking for a set of Pods.
- uploader Pods talking to the renderer Pods via a Kubernetes Service object. The Service (capital “S” because it’s a Kubernetes API object) is providing a reliable name and IP. It’s also load-balancing requests to the two renderer Pods behind it.
- kubectl (the Kubernetes client)
- run quite a lot of work on k3d on my laptop, and I highly recommend it if you want a multi-node Kubernetes dev environment on your laptop.
- kubectl converts user-friendly commands into HTTP REST requests with JSON content required by the Kubernetes API server. It uses a configuration file to know which cluster and API server endpoint to send commands to, and it takes care of sending authentication data with commands.
- Broadly speaking, there are three main reasons for Pods. Pods augment containers Pods assist in scheduling Pods enable resource sharing
- Pods deployed via controllers have all the benefits of being monitored and managed by a highly-available controller running on the control-plane. The local kubelet can still attempt local restarts, but if restart attempts fail, or the node itself fails, the observing controller can start a replacement Pod on a different worker node.
- Just to be clear, it’s vital to understand that Pods are mortal. When they die, they’re gone. There’s no fixing them and bringing them back from the dead. This firmly places them in the cattle category of the pets vs cattle paradigm. Pods are cattle, and when they die, they get replaced by another. There’s no tears and no funeral. The old one is gone, and a shiny new one – with the same config, but a different IP address and UID – magically appears and takes its place. This is why applications should always store state and data outside the Pod. It’s also why you shouldn’t rely on individual Pods – they’re ephemeral, here today, gone tomorrow…
- Shared execution environment means the Pod has a set of resources that are shared by every container it runs. These resources include IP address, ports, hostname, sockets, memory, volumes, and more…
- The pod network is flat, meaning every Pod can talk directly to every other Pod without the need for complex routing and port mappings.
- The immutable nature of Pods is a key aspect of cloud-native microservices design patterns and forces the following behaviors. When updates are needed, replace all old Pods with new ones that have the updates When failures occur, replace failed Pods with new ones
- if you need to scale the app, you add or remove Pods. This is called horizontal scaling.
- The following command shows a snipped output from of a kubectl get pods -o yaml command. The output is divided into two main parts: desired state (.spec) observed state (.status)
- Installing tools on running Pods and containers is an anti-pattern and should be avoided. If you need to make changes to a Pod, you should create a new Pod with the updates and replace old ones with the new one.
- Before going any further, it’s important to understand that most Kubernetes objects are deployed to a Namespace. These objects are said to be namespaced and include common objects such as Pods, Services and Deployments. Other objects exist outside of Namespaces and include nodes and PodSecurityPolicies.
- The Deployment spec is a declarative YAML object where you describe the desired state of a stateless app. You give it to Kubernetes where the Deployment controller implements and manages it. The controller element is highly-available and operates as a background loop, on the control plane, reconciling observed state with desired state.
- ReplicaSets manage Pods and bring self-healing and scaling. Deployments manage ReplicaSets and add rollouts and rollbacks. As a result, working with Deployments brings the benefits of everything else – the container, the Pod, the ReplicaSet.
- The declarative model is all about describing an end-goal – telling Kubernetes what you want. The imperative model is all about long lists of commands to reach an end-goal – telling Kubernetes how to do something.
- they require a couple of things from your microservices applications in order to work properly. Loose coupling via APIs Backwards and forwards compatibility Both of these are hallmarks of modern cloud-native microservices apps
- Don’t get confused if you hear rollbacks referred to as “updates”. That’s exactly what they are. A rollback follows exactly the same logic and rules as an update/rollout – terminate Pods with the current image and replace them with Pods running the new image. Just in the case of a rollback, the “new” image is actually an “older” one.
- First, when talking about Services with a capital “S”, we’re talking about the Service object in Kubernetes that provides stable networking for Pods. Just like a Pod, ReplicaSet, or Deployment, a Kubernetes Service is a REST object in the API that you define in a manifest file and post to the API server. Second, every Service gets its own stable IP address, its own stable DNS name, and its own stable port. Third, Services use labels and selectors to dynamically select the Pods they send traffic to.
- Services are loosely coupled with Pods via labels and selectors.
- Every time you create a Service, Kubernetes automatically creates an associated Endpoints object. This Endpoints object is used to store a dynamic list of healthy Pods matching the Service’s label selector.
- Recent versions of Kubernetes are replacing Endpoints objects with more efficient EndpointSlices. The functionality is identical, but EndpointSlices have better performance and are more efficient.
- ClusterIP Service has a stable virtual IP address that is only accessible from inside the cluster. It’s programmed into the internal network fabric and guaranteed to be stable for the life of the Service. Programmed into the network fabric is fancy way of saying the network just knows about it and you don’t need to bother with the details.
- NodePort Services build on top of the ClusterIP type and allow external clients to hit a dedicated port on every cluster node and reach the Service. We call this dedicated port the “NodePort”.
- LoadBalancer Services make external access even easier by integrating with an internet-facing load-balancer on your underlying cloud platform. You get a high-performance highly-available public IP or DNS name that you can access the Service from. You can even register friendly DNS names to make access even simpler – you don’t need to know cluster node names or IPs.
- Ingress is all about accessing multiple web applications through a single LoadBalancer Service.
- if you plan to run a service mesh, you may not need Ingress.
- The “OSI model” is the reference model for modern networking. It comprises seven layers numbered 1-7, with the lowest layers concerned with things like signalling and electronics, the middle layers dealing with reliability through things like acks and retries, and the higher layers adding awareness of user apps such as HTTP services. Ingress operates at layer 7, also known as the application layer, and implements HTTP intelligence.
- Service registration is the process of an application listing its connection details in a service registry so other apps can find it and consume it.
- However, no matter what type of storage, or where it comes from, when it’s exposed on Kubernetes it’s called a volume.
- the interface that connects external storage with Kubernetes. Modern plugins are be based on the Container Storage Interface (CSI) which is an open standard aimed at providing a clean storage interface for container orchestrators such as Kubernetes. If you’re a developer writing storage plugins, the CSI abstracts the internal Kubernetes machinery and lets you develop out-of-tree.
- Kubernetes persistent volume subsystem. This is a set of API objects that make it easy for applications to consume storage. There are a growing number of storage-related API objects, but the core ones are: Persistent Volumes (PV) Persistent Volume Claims (PVC) Storage Classes (SC)
- At a high level, PVs are how external storage assets are represented in Kubernetes. PVCs are like tickets that grant a Pod access to a PV. SCs make it all dynamic.
- In the past, we coupled the application and the configuration into a single easy-to-deploy unit. As we moved into the early days of cloud-native microservices we brought this model with us. However, it’s an anti-pattern and you should de-couple the application and the configuration. Doing this brings benefits such as: Re-usable application images Simpler development and testing Simpler and fewer disruptive changes
- You should not use ConfigMaps to store sensitive data such as certificates and passwords. Kubernetes provides a different object, called a Secret, for storing sensitive data. Secrets and ConfigMaps are very similar in design and implementation, the major difference is that Kubernetes takes steps to obscure the data stored in Secrets. It makes no such efforts to obscure data in ConfigMaps.
- Once data is stored in a ConfigMap, it can be injected into containers at run-time via any of the following methods: Environment variables Arguments to the container’s startup command Files in a volume
- Despite being designed for sensitive data, Kubernetes does not encrypt Secrets in the cluster store. It merely obscures them as base-64 encoded values that can easily be decoded. Fortunately, it’s possible to configure encryption-at-rest with EncryptionConfiguration objects, and most service meshes encrypt network traffic. Despite this, many people opt to use external 3rd-party tools, such as HashiCorp’s Vault, for a more complete and secure secrets management solution.
- failed Pods managed by a StatefulSet will be replaced by new Pods with the exact same Pod name, the exact same DNS hostname, and the exact same volumes. This is true even if the replacement is started on a different cluster node. The same is not true of Pods managed by a Deployment.
- StatefulSets create one Pod at a time, and always wait for previous Pods to be running and ready before creating the next. This is different from Deployments that use a ReplicaSet controller to start all Pods at the same time, causing potential race conditions.
- StatefulSet controllers do their own self-healing and scaling. This is architecturally different to Deployments which use a separate ReplicaSet controller for these operations.
- headless Service is just a regular Kubernetes Service object without an IP address (spec.clusterIP set to None). It becomes a StatefulSet’s governing Service when you list it in the StatefulSet config under spec.serviceName.
- By default, Kubernetes places all objects within the cluster.local DNS subdomain.
- All requests to the API server have to include credentials, and the authentication layer is responsible for verifying them. If verification fails, the API server returns an HTTP 401 and the request is denied. If authentication succeeds, the request moves on to authorization.
- The most common authorization module is RBAC (Role-Based Access Control). At the highest level, it’s about three things. Users Actions Resources Which users can perform which actions against which resources.
- Client certificates are commonly used, and integration with AD and other IAM services is recommended for production clusters.
- Serialization is the process of converting an object into a string, or stream of bytes, so it can be sent over a network and persisted to a data store. The reverse process of converting a string or stream of bytes into and object is deserialization.
- Kubernetes serializes objects, such as Pods and Services, as JSON strings to be sent over HTTP. The process happens in both directions, with clients like kubectl serializing objects when posting to the API server, and the API server serializing responses back to clients. In the case of Kubernetes, the serialised state of objects is also persisted to the cluster store which is usually based on the etcd database.
- The API server exposes the API over a secure RESTful interface using HTTPS. It acts as the front-end to the API and is a bit like Grand Central for Kubernetes – everything talks to everything else via REST API calls to the API server.
- The API is where all Kubernetes resources are defined. It’s large, modular, and RESTful.
- named groups is a collection of related resources. For example, the “apps” group is where all resources that manage application workloads such as Deployments, ReplicaSets, DaemonSets, and StatefulSets are defined. Likewise, the “networking.k8s.io” group is where Ingresses, Ingress Classes, Network Policies, and other network-related resources exist.
- While kubectl can be useful for getting API info, it’s often better to explore the API more directly using one of the following options. API development tools Commands like curl, wget, and Invoke-WebRequest Web browser The simplest way to do this, is to run a kubectl proxy command that exposes the API on your localhost adapter and handles all security and authentication.
- Kubernetes API is resource-based. This means everything in the API is a resource. It just so happens that most API resources, such as Pods, Services, and Ingresses are objects. However, some resources are lists, and an even smaller number are operations. However, most resources are objects, so we often use the terms “resource” and “object” to mean the same thing.
- The API server runs as a control plane service, and all internal and external clients interact with each other and the API, via the API server. This means your control plane needs to be highly available and high performance. If it’s not, you risk slow API response times or entirely losing access to the API. Also, all requests to the API server are authenticated, authorized, and protected by TLS.
- Threat modeling is the process of identifying vulnerabilities so you can put measures in place to prevent and mitigate them. This chapter introduces the popular STRIDE model and shows how it can be applied to Kubernetes. STRIDE defines six categories of potential threat: Spoofing Tampering Repudiation Information disclosure Denial of service Elevation of privilege
- Spoofing is pretending to be somebody else with the aim of gaining extra privileges on a system.
- Tampering is the act of changing something, in a malicious way, so you can cause one of the following. Denial of service. Tampering with the resource to make it unusable. Elevation of privilege. Tampering with a resource to gain additional privileges.
- TLS is a great tool for protecting against in transit tampering, as it provides built-in integrity guarantees – you’ll be warned if the data has been tampered with.
- Repudiation At a very high level, repudiation is creating doubt about something. Non-repudiation is providing proof about something. In the context of information security, non-repudiation is proving certain actions were carried out by certain individuals.
- Information disclosure is when sensitive data is leaked.
- As a minimum, you should limit and audit access to the nodes hosting the cluster store.
- Denial of Service (DoS) is all about making something unavailable. There are many types of DoS attack, but a well-known variation is overloading a system to the point it can no longer service requests.
- A default installation of Kubernetes installs etcd on the same servers as the rest of the control plane. This is usually fine for development and testing, however, large production clusters should seriously consider a dedicated etcd cluster. This will provide better performance and greater resilience. On the performance front, etcd is probably the most common choking point for large Kubernetes clusters.
- Privilege escalation is gaining higher access than what is granted, usually in order to cause damage or gain unauthorized access.
- soft multi-tenancy is hosting multiple trusted workloads on shared infrastructure. By trusted, we mean workloads that don’t require absolute guarantees that one Pod/container cannot impact another.
- hard multi-tenancy as hosting untrusted and potentially hostile workloads on shared infrastructure.