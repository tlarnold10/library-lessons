# Continuous Delivery
__By: Jez Humble__
## Lessons Learned: 
- Acceptance testing can consist of automated and manual tests. They should be expressed from the perspective of the user, not the development team. For example, when you click this button, the purchase order is accepted. Automated acceptance tests should include a step for setup, test, and tear down.
- Smoke tests for deployment. Makes sure that the application works as expected after it is installed. Can be as simple as opening up the application and making sure it looks okay. I wonder if we can come up with some automated checks for the environmental  variables and services to make sure everything is good to go.
- Draw out the entire process from customer request to feature in their hand. Pay special attention to the parts of the process and the parts that don't ( wait time)
- In general, for each story or requirement there is a single path though the application in terms of the auctions the user will perform. This is known as the happy path. Deviations from this path would be considered alternative paths. Finally you have to take into consideration the sad path, which would cause errors to occur. You should consider this different paths where creating the functional tests for the story
- Three different types of automated tests: unit tests, component tests, and acceptance tests. Look at Brian Marick’s testing quadrant diagram. This diagram elevates different tests on their automation level and whether they are business facing or developer facing.
- Continuous integration requires that every time somebody commits any changes, the entire application is build and a comprehensive set of automated tests are ran against it. If the build or tests fail, the development team stops what they are doing and fixes the problems immediately. The goal of continuous integration is that the software is always in a working state.
- Config information can be used at 4 different times: build time, packaging time, deployment time, or startup or ran time. Generally should not use it at build or packaging time.
- If something is painful in the process, start doing it more frequently. Instead of pushing the challenge to the end of the process, move it up and work on getting better on that specific thing
- Every change should be a valid release candidate. Every time a change is committed to version control, the expectation is that it will pass all of its tests, produce working code, and could be released into production
- Quality does not mean perfect. Voltaire said, " The perfect is the enemy of good” - but our goal should always be to deliver software of sufficient qualify to bring value to users. The goal is to find ways to deliver high quality , valuable software in an efficient, fast, and reliable manner.
- Even when working on a long running feature in development, continue to release often. Just turn off that functionality until it is ready to be used.it is tempting to branch, but don't do it. The bigger the apparent reason to branch, the more you should not branch. You could also create an abstraction layer: create an abstraction layer over the part that needs to be changed, refactor the rest of the system to use the abstraction layer, create a new implementation that is not part of the production code path, update abstraction layer to delegate to your new implementation, and then remove the old implementation
- Root cause analysis, continuously ask "why". Usually will need to ask "why" at least five times in order to come to the root cause.
- You should keep a hash of each artifact so you can verity the source of any given binary. You need to be able to map the binary to the version from source control that was used to create it.
- Minimize the number of builds you operate
- Libraries are packages your team does not control, while components are dependencies that your team has developed. Build time dependencies must be present when your application is compiled, while run time dependencies must be present when the application runs.
- Decoupling changes to the software and changes to the database can help the upgrade process. Consider using an abstraction layer in the database, such as views and stored procedures.
- There are four areas to consider when creating a monitoring strategy: instrumenting your applications and your infrastructure so you can collect the data you need, storing the data so it can easily be retrieved, creating dashboards with aggregate data for operations and the business, and setting up notifications.
- Infrastructure as a service is like AWS providing cloud computing services , think EC2. Platform as a service takes away some of the flexibility in order to provide something that has some of the bits built for you already, think Prismatic.
- A good question to ask, and test, is: How long would it take to provision a new copy of my production environment if it failed?
- Even if you don't release every change you make right when you make it, you should behave as if you were going to.
- A zero downtime release, also known as a hot deployment. Key is decoupling the various parts of the release, like databases, services, and static resources, before you upgrade the application.
- A production like environment has the following characteristics: same OS as production, same software installed, and it should be managed in the same way
- Create a simple smoke test after deployment that verifies that the deployment worked and the software is running as expected
- Some types of measurements that can be performed: scalability testing, longevity testing, throughput, load testing. Scenario based testing represent a specific scenario of use of the system as a test, and evaluate that against our business predictions of what it must achieve in the real world..
- Before a solution can be found, the source of the problem has to be identified. before that happens, we need to know that we have a problem at all. The point of the explicitly testing is to tell us whether we have a problem, so that we can go on to fix it. Don't guess, measure. Writing performant code can be messy. Initially, you should focus on writing maintainable code, at least until you have a test that points out the specific problem.
- The application drive layer is the layer that understands how to talk to your application, the system under test. The API for the application drives layer is expressed in a domain language, and indeed can be thought of as a domain specific language of its own right.
- Behavior-driven development. The core idea that your acceptance  criteria should be written in the form of a customer's expectations of the behavior of the application.
- Acceptance criteria should define the feature value to the user. Testers should be involved in defining  the acceptance criteria. Follow the form: given some initial context, when an event occurs, then there are some outcomes. Write an implementation for the test which uses only the domain language, accessing the application driver layer
- Unit tests should be fast and cover about 80% of the code base. Running unit tests should not touch the file system, database, libraries, frameworks or external systems. Use stubbing, which is replacing a part of the system with a simulated version of that system. Mocking then builds on top of stubbing. If your tests look cumbersome and complex, it reflects on the design of your system.
- Manual testing should still be done, but it should focus on the thing that humans are better than computers at, such as: exploratory testing, perform user testing of the apps usability check the look and feel on various platforms, and carry out pathological worst-case tests..